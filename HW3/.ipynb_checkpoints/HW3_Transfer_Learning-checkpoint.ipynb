{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load data: digits 5 to 9, but still label with 0 to 4, \n",
    "# because TensorFlow expects label's integers from 0 to n_classes-1.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to keep only 100 instances per class in the training set \n",
    "# and let's keep only 30 instances per class in the validation set\n",
    "# tesing set is already loaded above\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs   = 1000         # Spec 要求 1000 epochs\n",
    "early_stop_limit = 20     # Spec 要求，利用 early stop 節省時間\n",
    "batch_size = 20           # 設定 batch，節省 GPU memory 使用率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "用 function 取代重複的部分，也就是訓練的過程，傳入 training dataset 和 validation dataset 以及 cache 參數來決定是否快取\n",
    "快取的部分是針對 feed_dict 做更動，如果啟用快取功能，則 feed_dict 會直接輸入第五層結果，若無則是 X, y。\n",
    "\"\"\"\n",
    "def run(sess, train_data, train_label, X_valid, y_valid, cache=False):\n",
    "    best_loss = np.infty # 預設無限大，後面訓練過程若有更小值則取代\n",
    "    early_stop = 0       # 紀錄 early_stop 的次數\n",
    "    # summary_writer = tf.summary.FileWriter('tensorboard/', graph=tf.get_default_graph())\n",
    "\n",
    "    # 利用 cache 判斷是否要做快取，因為 sess.run 的部分都相同，因此針對 feed_dict 來做更動\n",
    "    if cache:\n",
    "        hidden5_train = sess.run(hidden5_out, feed_dict={X: train_data})\n",
    "        hidden5_valid = sess.run(hidden5_out, feed_dict={X: X_valid})\n",
    "        val_feed = {hidden5_out: hidden5_valid, y: y_valid}\n",
    "    else:\n",
    "        val_feed = {X: X_valid, y: y_valid}\n",
    "        \n",
    "    # 1000 epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # 使用 batch 來讓執行的過程使用較小的 memory\n",
    "        rnd_idx = np.random.permutation(len(train_data))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(train_data) // batch_size):\n",
    "            \n",
    "            # 利用 cache 判斷是否要做快取，因為 sess.run 的部分都相同，因此針對 feed_dict 來做更動\n",
    "            if cache:\n",
    "                train_feed = {hidden5_out: hidden5_train[rnd_indices], y: train_label[rnd_indices]}\n",
    "            else:\n",
    "                train_feed = {X: train_data[rnd_indices], y: train_label[rnd_indices]}\n",
    "            sess.run(training_op, feed_dict=train_feed)\n",
    "\n",
    "             \n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict=val_feed)\n",
    "        \n",
    "        # 設定 early stop 機制讓已經收斂的 model 提早結束，節省時間\n",
    "        if loss_val < best_loss:\n",
    "            saver.save(sess, checkpoint_dir)\n",
    "            best_loss = loss_val\n",
    "            early_stop = 0\n",
    "        else:\n",
    "            early_stop += 1\n",
    "            if early_stop > early_stop_limit:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "                \n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####\n",
    "#\n",
    "#     Homework 3.1 Softmax only\n",
    "#\n",
    "#\n",
    "#     基本上結果會是：Final test accuracy: 84.80% 上下，時間約 13 秒左右。\n",
    "#\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"save/Team07_HW2.ckpt.meta\")            # 回復 HW2 完成的 graph 及其 model\n",
    "\n",
    "X         = tf.get_default_graph().get_tensor_by_name(\"X:0\")                       # 利用 name 抓出 tf 變數\n",
    "y         = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss      = tf.get_default_graph().get_tensor_by_name(\"calc_loss/loss:0\")          # 因為在 HW2 中有用到 with scope，因此有前綴\n",
    "Y_proba   = tf.get_default_graph().get_tensor_by_name(\"DNN/Y_proba:0\")\n",
    "logits    = Y_proba.op.inputs[0]\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\") # 可從所有變數中抓出可以更新（訓練）的變數，利用 scope 抓出該層的變數\n",
    "optimizer         = tf.train.AdamOptimizer(learning_rate=0.01, name=\"Adam2\")\n",
    "training_op       = optimizer.minimize(loss, var_list=output_layer_vars)                # 再從 optimizer 這裡限制要被更新的變數有哪些\n",
    "\n",
    "correct  = tf.equal(tf.argmax(logits, 1), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "saver  = tf.train.Saver()\n",
    "config = tf.ConfigProto(device_count = {'GPU': 1}) # 指定 GPU id，限制使用它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/Team07_HW2.ckpt\n",
      "0\tValidation loss: 1.305937\tBest loss: 1.305937\tAccuracy: 59.33%\n",
      "1\tValidation loss: 1.257764\tBest loss: 1.257764\tAccuracy: 63.33%\n",
      "2\tValidation loss: 1.150286\tBest loss: 1.150286\tAccuracy: 75.33%\n",
      "3\tValidation loss: 1.128988\tBest loss: 1.128988\tAccuracy: 77.33%\n",
      "4\tValidation loss: 1.103042\tBest loss: 1.103042\tAccuracy: 80.67%\n",
      "5\tValidation loss: 1.108267\tBest loss: 1.103042\tAccuracy: 80.00%\n",
      "6\tValidation loss: 1.070573\tBest loss: 1.070573\tAccuracy: 82.00%\n",
      "7\tValidation loss: 1.059471\tBest loss: 1.059471\tAccuracy: 85.33%\n",
      "8\tValidation loss: 1.066965\tBest loss: 1.059471\tAccuracy: 85.33%\n",
      "9\tValidation loss: 1.091169\tBest loss: 1.059471\tAccuracy: 83.33%\n",
      "10\tValidation loss: 1.076296\tBest loss: 1.059471\tAccuracy: 83.33%\n",
      "11\tValidation loss: 1.057558\tBest loss: 1.057558\tAccuracy: 85.33%\n",
      "12\tValidation loss: 1.058213\tBest loss: 1.057558\tAccuracy: 85.33%\n",
      "13\tValidation loss: 1.066885\tBest loss: 1.057558\tAccuracy: 84.00%\n",
      "14\tValidation loss: 1.062223\tBest loss: 1.057558\tAccuracy: 86.00%\n",
      "15\tValidation loss: 1.063300\tBest loss: 1.057558\tAccuracy: 84.67%\n",
      "16\tValidation loss: 1.056838\tBest loss: 1.056838\tAccuracy: 84.00%\n",
      "17\tValidation loss: 1.058395\tBest loss: 1.056838\tAccuracy: 83.33%\n",
      "18\tValidation loss: 1.058321\tBest loss: 1.056838\tAccuracy: 84.67%\n",
      "19\tValidation loss: 1.069282\tBest loss: 1.056838\tAccuracy: 83.33%\n",
      "20\tValidation loss: 1.054143\tBest loss: 1.054143\tAccuracy: 85.33%\n",
      "21\tValidation loss: 1.068801\tBest loss: 1.054143\tAccuracy: 82.67%\n",
      "22\tValidation loss: 1.060130\tBest loss: 1.054143\tAccuracy: 83.33%\n",
      "23\tValidation loss: 1.049073\tBest loss: 1.049073\tAccuracy: 85.33%\n",
      "24\tValidation loss: 1.051727\tBest loss: 1.049073\tAccuracy: 84.67%\n",
      "25\tValidation loss: 1.051283\tBest loss: 1.049073\tAccuracy: 85.33%\n",
      "26\tValidation loss: 1.051222\tBest loss: 1.049073\tAccuracy: 85.33%\n",
      "27\tValidation loss: 1.060728\tBest loss: 1.049073\tAccuracy: 83.33%\n",
      "28\tValidation loss: 1.069062\tBest loss: 1.049073\tAccuracy: 82.67%\n",
      "29\tValidation loss: 1.064728\tBest loss: 1.049073\tAccuracy: 82.67%\n",
      "30\tValidation loss: 1.072237\tBest loss: 1.049073\tAccuracy: 83.33%\n",
      "31\tValidation loss: 1.053247\tBest loss: 1.049073\tAccuracy: 84.00%\n",
      "32\tValidation loss: 1.053869\tBest loss: 1.049073\tAccuracy: 85.33%\n",
      "33\tValidation loss: 1.061097\tBest loss: 1.049073\tAccuracy: 83.33%\n",
      "34\tValidation loss: 1.066567\tBest loss: 1.049073\tAccuracy: 84.00%\n",
      "35\tValidation loss: 1.055896\tBest loss: 1.049073\tAccuracy: 84.67%\n",
      "36\tValidation loss: 1.057434\tBest loss: 1.049073\tAccuracy: 84.00%\n",
      "37\tValidation loss: 1.058434\tBest loss: 1.049073\tAccuracy: 83.33%\n",
      "38\tValidation loss: 1.055486\tBest loss: 1.049073\tAccuracy: 84.67%\n",
      "39\tValidation loss: 1.053932\tBest loss: 1.049073\tAccuracy: 84.00%\n",
      "40\tValidation loss: 1.053027\tBest loss: 1.049073\tAccuracy: 86.00%\n",
      "41\tValidation loss: 1.047919\tBest loss: 1.047919\tAccuracy: 84.67%\n",
      "42\tValidation loss: 1.050887\tBest loss: 1.047919\tAccuracy: 84.67%\n",
      "43\tValidation loss: 1.048431\tBest loss: 1.047919\tAccuracy: 85.33%\n",
      "44\tValidation loss: 1.057211\tBest loss: 1.047919\tAccuracy: 84.00%\n",
      "45\tValidation loss: 1.055207\tBest loss: 1.047919\tAccuracy: 84.67%\n",
      "46\tValidation loss: 1.052374\tBest loss: 1.047919\tAccuracy: 83.33%\n",
      "47\tValidation loss: 1.054612\tBest loss: 1.047919\tAccuracy: 83.33%\n",
      "48\tValidation loss: 1.057182\tBest loss: 1.047919\tAccuracy: 83.33%\n",
      "49\tValidation loss: 1.058190\tBest loss: 1.047919\tAccuracy: 83.33%\n",
      "50\tValidation loss: 1.059142\tBest loss: 1.047919\tAccuracy: 83.33%\n",
      "51\tValidation loss: 1.058842\tBest loss: 1.047919\tAccuracy: 82.67%\n",
      "52\tValidation loss: 1.057546\tBest loss: 1.047919\tAccuracy: 83.33%\n",
      "53\tValidation loss: 1.060984\tBest loss: 1.047919\tAccuracy: 82.67%\n",
      "54\tValidation loss: 1.058900\tBest loss: 1.047919\tAccuracy: 82.67%\n",
      "55\tValidation loss: 1.059288\tBest loss: 1.047919\tAccuracy: 83.33%\n",
      "56\tValidation loss: 1.058149\tBest loss: 1.047919\tAccuracy: 81.33%\n",
      "57\tValidation loss: 1.059302\tBest loss: 1.047919\tAccuracy: 83.33%\n",
      "58\tValidation loss: 1.060729\tBest loss: 1.047919\tAccuracy: 82.00%\n",
      "59\tValidation loss: 1.062568\tBest loss: 1.047919\tAccuracy: 83.33%\n",
      "60\tValidation loss: 1.060884\tBest loss: 1.047919\tAccuracy: 82.67%\n",
      "61\tValidation loss: 1.059660\tBest loss: 1.047919\tAccuracy: 83.33%\n",
      "Early stopping!\n",
      "Total training time: 13.7s\n",
      "INFO:tensorflow:Restoring parameters from save/Team07_HW3_1.ckpt\n",
      "Final test accuracy: 84.80%\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = 'save/Team07_HW3_1.ckpt' # 新的 model 儲存位置\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    restore_saver.restore(sess, \"save/Team07_HW2.ckpt\") # 從 HW2 存的 ckpt 中回復訓練好的變數\n",
    "    for var in output_layer_vars:                       # 針對會被更新的變數做 initial\n",
    "        var.initializer.run()\n",
    "\n",
    "    t0 = time.time()\n",
    "    run(sess, X_train2, y_train2, X_valid2, y_valid2, False) # 開始訓練，不使用 cache\n",
    "    \n",
    "    print(\"Total training time: {:.1f}s\".format(time.time() - t0))\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, checkpoint_dir) # 從訓練過程中儲存的 ckpt 取回 model\n",
    "    acc_test = sess.run(accuracy, feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### \n",
    "#\n",
    "#     Homework 3.2 Cache\n",
    "#\n",
    "#\n",
    "#     基本上結果一樣會是：Final test accuracy: 84.80% 上下，而時間約快 0.2 - 0.5 秒，\n",
    "#     因為 early stop 的原因，epoch 次數不會太多，因此時間差距不會被凸顯出來，\n",
    "#     若是到 1000 epochs ，則可以快至少 6 秒。\n",
    "#\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"save/Team07_HW2.ckpt.meta\")            # 回復 HW2 完成的 graph 及其 model\n",
    "\n",
    "X         = tf.get_default_graph().get_tensor_by_name(\"X:0\")                       # 利用 name 抓出 tf 變數\n",
    "y         = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss      = tf.get_default_graph().get_tensor_by_name(\"calc_loss/loss:0\")          # 因為在 HW2 中有用到 with scope，因此有前綴\n",
    "Y_proba   = tf.get_default_graph().get_tensor_by_name(\"DNN/Y_proba:0\")\n",
    "logits    = Y_proba.op.inputs[0]\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\") # 可從所有變數中抓出可以更新（訓練）的變數，利用 scope 抓出該層的變數\n",
    "optimizer         = tf.train.AdamOptimizer(learning_rate=0.01, name=\"Adam2\")\n",
    "training_op       = optimizer.minimize(loss, var_list=output_layer_vars)                # 再從 optimizer 這裡限制要被更新的變數有哪些\n",
    "\n",
    "correct  = tf.equal(tf.argmax(logits, 1), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "saver  = tf.train.Saver()\n",
    "config = tf.ConfigProto(device_count = {'GPU': 1}) # 指定 GPU id，限制使用它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"DNN/hidden5/Elu:0\")       # 第1-5層因為都不會更動，所以可直接拿第五層的結果做 softmax 層的計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/Team07_HW2.ckpt\n",
      "0\tValidation loss: 1.305937\tBest loss: 1.305937\tAccuracy: 59.33%\n",
      "1\tValidation loss: 1.257764\tBest loss: 1.257764\tAccuracy: 63.33%\n",
      "2\tValidation loss: 1.150286\tBest loss: 1.150286\tAccuracy: 75.33%\n",
      "3\tValidation loss: 1.128988\tBest loss: 1.128988\tAccuracy: 77.33%\n",
      "4\tValidation loss: 1.103041\tBest loss: 1.103041\tAccuracy: 80.67%\n",
      "5\tValidation loss: 1.108267\tBest loss: 1.103041\tAccuracy: 80.00%\n",
      "6\tValidation loss: 1.070573\tBest loss: 1.070573\tAccuracy: 82.00%\n",
      "7\tValidation loss: 1.059471\tBest loss: 1.059471\tAccuracy: 85.33%\n",
      "8\tValidation loss: 1.066965\tBest loss: 1.059471\tAccuracy: 85.33%\n",
      "9\tValidation loss: 1.091168\tBest loss: 1.059471\tAccuracy: 83.33%\n",
      "10\tValidation loss: 1.076296\tBest loss: 1.059471\tAccuracy: 83.33%\n",
      "11\tValidation loss: 1.057558\tBest loss: 1.057558\tAccuracy: 85.33%\n",
      "12\tValidation loss: 1.058213\tBest loss: 1.057558\tAccuracy: 85.33%\n",
      "13\tValidation loss: 1.066885\tBest loss: 1.057558\tAccuracy: 84.00%\n",
      "14\tValidation loss: 1.062223\tBest loss: 1.057558\tAccuracy: 86.00%\n",
      "15\tValidation loss: 1.063300\tBest loss: 1.057558\tAccuracy: 84.67%\n",
      "16\tValidation loss: 1.056838\tBest loss: 1.056838\tAccuracy: 84.00%\n",
      "17\tValidation loss: 1.058395\tBest loss: 1.056838\tAccuracy: 83.33%\n",
      "18\tValidation loss: 1.058321\tBest loss: 1.056838\tAccuracy: 84.67%\n",
      "19\tValidation loss: 1.069282\tBest loss: 1.056838\tAccuracy: 83.33%\n",
      "20\tValidation loss: 1.054143\tBest loss: 1.054143\tAccuracy: 85.33%\n",
      "21\tValidation loss: 1.068801\tBest loss: 1.054143\tAccuracy: 82.67%\n",
      "22\tValidation loss: 1.060130\tBest loss: 1.054143\tAccuracy: 83.33%\n",
      "23\tValidation loss: 1.049073\tBest loss: 1.049073\tAccuracy: 85.33%\n",
      "24\tValidation loss: 1.051728\tBest loss: 1.049073\tAccuracy: 84.67%\n",
      "25\tValidation loss: 1.051283\tBest loss: 1.049073\tAccuracy: 85.33%\n",
      "26\tValidation loss: 1.051222\tBest loss: 1.049073\tAccuracy: 85.33%\n",
      "27\tValidation loss: 1.060728\tBest loss: 1.049073\tAccuracy: 83.33%\n",
      "28\tValidation loss: 1.069062\tBest loss: 1.049073\tAccuracy: 82.67%\n",
      "29\tValidation loss: 1.064728\tBest loss: 1.049073\tAccuracy: 82.67%\n",
      "30\tValidation loss: 1.072237\tBest loss: 1.049073\tAccuracy: 83.33%\n",
      "31\tValidation loss: 1.053247\tBest loss: 1.049073\tAccuracy: 84.00%\n",
      "32\tValidation loss: 1.053869\tBest loss: 1.049073\tAccuracy: 85.33%\n",
      "33\tValidation loss: 1.061097\tBest loss: 1.049073\tAccuracy: 83.33%\n",
      "34\tValidation loss: 1.066566\tBest loss: 1.049073\tAccuracy: 84.00%\n",
      "35\tValidation loss: 1.055896\tBest loss: 1.049073\tAccuracy: 84.67%\n",
      "36\tValidation loss: 1.057433\tBest loss: 1.049073\tAccuracy: 84.00%\n",
      "37\tValidation loss: 1.058434\tBest loss: 1.049073\tAccuracy: 83.33%\n",
      "38\tValidation loss: 1.055487\tBest loss: 1.049073\tAccuracy: 84.67%\n",
      "39\tValidation loss: 1.053932\tBest loss: 1.049073\tAccuracy: 84.00%\n",
      "40\tValidation loss: 1.053027\tBest loss: 1.049073\tAccuracy: 86.00%\n",
      "41\tValidation loss: 1.047920\tBest loss: 1.047920\tAccuracy: 84.67%\n",
      "42\tValidation loss: 1.050887\tBest loss: 1.047920\tAccuracy: 84.67%\n",
      "43\tValidation loss: 1.048432\tBest loss: 1.047920\tAccuracy: 85.33%\n",
      "44\tValidation loss: 1.057211\tBest loss: 1.047920\tAccuracy: 84.00%\n",
      "45\tValidation loss: 1.055206\tBest loss: 1.047920\tAccuracy: 84.67%\n",
      "46\tValidation loss: 1.052373\tBest loss: 1.047920\tAccuracy: 83.33%\n",
      "47\tValidation loss: 1.054612\tBest loss: 1.047920\tAccuracy: 83.33%\n",
      "48\tValidation loss: 1.057183\tBest loss: 1.047920\tAccuracy: 83.33%\n",
      "49\tValidation loss: 1.058190\tBest loss: 1.047920\tAccuracy: 83.33%\n",
      "50\tValidation loss: 1.059142\tBest loss: 1.047920\tAccuracy: 83.33%\n",
      "51\tValidation loss: 1.058842\tBest loss: 1.047920\tAccuracy: 82.67%\n",
      "52\tValidation loss: 1.057546\tBest loss: 1.047920\tAccuracy: 83.33%\n",
      "53\tValidation loss: 1.060984\tBest loss: 1.047920\tAccuracy: 82.67%\n",
      "54\tValidation loss: 1.058900\tBest loss: 1.047920\tAccuracy: 82.67%\n",
      "55\tValidation loss: 1.059288\tBest loss: 1.047920\tAccuracy: 83.33%\n",
      "56\tValidation loss: 1.058149\tBest loss: 1.047920\tAccuracy: 81.33%\n",
      "57\tValidation loss: 1.059302\tBest loss: 1.047920\tAccuracy: 83.33%\n",
      "58\tValidation loss: 1.060729\tBest loss: 1.047920\tAccuracy: 82.00%\n",
      "59\tValidation loss: 1.062568\tBest loss: 1.047920\tAccuracy: 83.33%\n",
      "60\tValidation loss: 1.060883\tBest loss: 1.047920\tAccuracy: 82.67%\n",
      "61\tValidation loss: 1.059660\tBest loss: 1.047920\tAccuracy: 83.33%\n",
      "Early stopping!\n",
      "Total training time: 13.2s\n",
      "INFO:tensorflow:Restoring parameters from save/Team07_HW3_2.ckpt\n",
      "Final test accuracy: 84.80%\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir=\"save/Team07_HW3_2.ckpt\"\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    restore_saver.restore(sess, \"save/Team07_HW2.ckpt\") # 從 HW2 存的 ckpt 中回復訓練好的變數\n",
    "    for var in output_layer_vars:                       # 針對會被更新的變數做 initial\n",
    "        var.initializer.run()\n",
    "\n",
    "    t0 = time.time()\n",
    "    run(sess, X_train2, y_train2, X_valid2, y_valid2, True) # 開始訓練，不使用 cache\n",
    "    \n",
    "    print(\"Total training time: {:.1f}s\".format(time.time() - t0))\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, checkpoint_dir) # 從訓練過程中儲存的 ckpt 取回 model\n",
    "    acc_test = sess.run(accuracy, feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####\n",
    "#\n",
    "#     Homework 3.3 4 layer instead\n",
    "#\n",
    "#\n",
    "#     基本上結果會是：Final test accuracy: 87.22% 上下，進步 3%。\n",
    "#\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"save/Team07_HW2.ckpt.meta\")            # 回復 HW2 完成的 graph 及其 model\n",
    "\n",
    "X         = tf.get_default_graph().get_tensor_by_name(\"X:0\")                       # 利用 name 抓出 tf 變數\n",
    "y         = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "hidden4   = tf.get_default_graph().get_tensor_by_name(\"DNN/hidden4/Elu:0\")\n",
    "logits    = tf.layers.dense(hidden4, 5, name=\"new_logits\")\n",
    "Y_proba   = tf.nn.softmax(logits)\n",
    "\n",
    "entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\") # 可從所有變數中抓出可以更新（訓練）的變數，利用 scope 抓出該層的變數\n",
    "optimizer         = tf.train.AdamOptimizer(learning_rate=0.01, name=\"Adam2\")\n",
    "training_op       = optimizer.minimize(loss, var_list=output_layer_vars)                # 再從 optimizer 這裡限制要被更新的變數有哪些\n",
    "\n",
    "correct  = tf.equal(tf.argmax(logits, 1), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "saver  = tf.train.Saver()\n",
    "config = tf.ConfigProto(device_count = {'GPU': 1}) # 指定 GPU id，限制使用它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/Team07_HW2.ckpt\n",
      "0\tValidation loss: 0.680522\tBest loss: 0.680522\tAccuracy: 75.33%\n",
      "1\tValidation loss: 0.516078\tBest loss: 0.516078\tAccuracy: 82.67%\n",
      "2\tValidation loss: 0.476066\tBest loss: 0.476066\tAccuracy: 84.00%\n",
      "3\tValidation loss: 0.417379\tBest loss: 0.417379\tAccuracy: 86.00%\n",
      "4\tValidation loss: 0.405972\tBest loss: 0.405972\tAccuracy: 86.00%\n",
      "5\tValidation loss: 0.425633\tBest loss: 0.405972\tAccuracy: 86.00%\n",
      "6\tValidation loss: 0.381811\tBest loss: 0.381811\tAccuracy: 89.33%\n",
      "7\tValidation loss: 0.401535\tBest loss: 0.381811\tAccuracy: 86.67%\n",
      "8\tValidation loss: 0.404661\tBest loss: 0.381811\tAccuracy: 86.67%\n",
      "9\tValidation loss: 0.395030\tBest loss: 0.381811\tAccuracy: 86.00%\n",
      "10\tValidation loss: 0.383258\tBest loss: 0.381811\tAccuracy: 86.67%\n",
      "11\tValidation loss: 0.377392\tBest loss: 0.377392\tAccuracy: 89.33%\n",
      "12\tValidation loss: 0.384404\tBest loss: 0.377392\tAccuracy: 88.67%\n",
      "13\tValidation loss: 0.386250\tBest loss: 0.377392\tAccuracy: 88.67%\n",
      "14\tValidation loss: 0.407817\tBest loss: 0.377392\tAccuracy: 88.67%\n",
      "15\tValidation loss: 0.362058\tBest loss: 0.362058\tAccuracy: 88.67%\n",
      "16\tValidation loss: 0.367198\tBest loss: 0.362058\tAccuracy: 88.00%\n",
      "17\tValidation loss: 0.372300\tBest loss: 0.362058\tAccuracy: 88.00%\n",
      "18\tValidation loss: 0.348135\tBest loss: 0.348135\tAccuracy: 88.67%\n",
      "19\tValidation loss: 0.366678\tBest loss: 0.348135\tAccuracy: 88.00%\n",
      "20\tValidation loss: 0.354889\tBest loss: 0.348135\tAccuracy: 88.67%\n",
      "21\tValidation loss: 0.366616\tBest loss: 0.348135\tAccuracy: 88.67%\n",
      "22\tValidation loss: 0.364982\tBest loss: 0.348135\tAccuracy: 87.33%\n",
      "23\tValidation loss: 0.355174\tBest loss: 0.348135\tAccuracy: 88.67%\n",
      "24\tValidation loss: 0.362942\tBest loss: 0.348135\tAccuracy: 88.00%\n",
      "25\tValidation loss: 0.370654\tBest loss: 0.348135\tAccuracy: 88.00%\n",
      "26\tValidation loss: 0.402225\tBest loss: 0.348135\tAccuracy: 88.00%\n",
      "27\tValidation loss: 0.439172\tBest loss: 0.348135\tAccuracy: 87.33%\n",
      "28\tValidation loss: 0.374027\tBest loss: 0.348135\tAccuracy: 87.33%\n",
      "29\tValidation loss: 0.392337\tBest loss: 0.348135\tAccuracy: 86.67%\n",
      "30\tValidation loss: 0.356591\tBest loss: 0.348135\tAccuracy: 88.00%\n",
      "31\tValidation loss: 0.355725\tBest loss: 0.348135\tAccuracy: 90.00%\n",
      "32\tValidation loss: 0.387523\tBest loss: 0.348135\tAccuracy: 87.33%\n",
      "33\tValidation loss: 0.380787\tBest loss: 0.348135\tAccuracy: 88.67%\n",
      "34\tValidation loss: 0.414782\tBest loss: 0.348135\tAccuracy: 86.67%\n",
      "35\tValidation loss: 0.358781\tBest loss: 0.348135\tAccuracy: 88.00%\n",
      "36\tValidation loss: 0.351182\tBest loss: 0.348135\tAccuracy: 88.67%\n",
      "37\tValidation loss: 0.368535\tBest loss: 0.348135\tAccuracy: 89.33%\n",
      "38\tValidation loss: 0.360944\tBest loss: 0.348135\tAccuracy: 88.67%\n",
      "39\tValidation loss: 0.346336\tBest loss: 0.346336\tAccuracy: 88.00%\n",
      "40\tValidation loss: 0.387101\tBest loss: 0.346336\tAccuracy: 87.33%\n",
      "41\tValidation loss: 0.368852\tBest loss: 0.346336\tAccuracy: 88.00%\n",
      "42\tValidation loss: 0.388963\tBest loss: 0.346336\tAccuracy: 87.33%\n",
      "43\tValidation loss: 0.375695\tBest loss: 0.346336\tAccuracy: 87.33%\n",
      "44\tValidation loss: 0.385658\tBest loss: 0.346336\tAccuracy: 87.33%\n",
      "45\tValidation loss: 0.371435\tBest loss: 0.346336\tAccuracy: 88.67%\n",
      "46\tValidation loss: 0.362792\tBest loss: 0.346336\tAccuracy: 87.33%\n",
      "47\tValidation loss: 0.353684\tBest loss: 0.346336\tAccuracy: 89.33%\n",
      "48\tValidation loss: 0.379344\tBest loss: 0.346336\tAccuracy: 88.67%\n",
      "49\tValidation loss: 0.376514\tBest loss: 0.346336\tAccuracy: 87.33%\n",
      "50\tValidation loss: 0.447899\tBest loss: 0.346336\tAccuracy: 86.67%\n",
      "51\tValidation loss: 0.428770\tBest loss: 0.346336\tAccuracy: 88.00%\n",
      "52\tValidation loss: 0.397351\tBest loss: 0.346336\tAccuracy: 87.33%\n",
      "53\tValidation loss: 0.357893\tBest loss: 0.346336\tAccuracy: 89.33%\n",
      "54\tValidation loss: 0.377080\tBest loss: 0.346336\tAccuracy: 88.00%\n",
      "55\tValidation loss: 0.392476\tBest loss: 0.346336\tAccuracy: 87.33%\n",
      "56\tValidation loss: 0.403549\tBest loss: 0.346336\tAccuracy: 88.00%\n",
      "57\tValidation loss: 0.397673\tBest loss: 0.346336\tAccuracy: 89.33%\n",
      "58\tValidation loss: 0.388368\tBest loss: 0.346336\tAccuracy: 88.67%\n",
      "59\tValidation loss: 0.404482\tBest loss: 0.346336\tAccuracy: 86.67%\n",
      "Early stopping!\n",
      "Total training time: 11.7s\n",
      "INFO:tensorflow:Restoring parameters from save/Team07_HW3_3.ckpt\n",
      "Final test accuracy: 87.22%\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir=\"save/Team07_HW3_3.ckpt\"\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    restore_saver.restore(sess, \"save/Team07_HW2.ckpt\") # 從 HW2 存的 ckpt 中回復訓練好的變數\n",
    "    for var in output_layer_vars:                       # 針對會被更新的變數做 initial\n",
    "        var.initializer.run()\n",
    "\n",
    "    t0 = time.time()\n",
    "    run(sess, X_train2, y_train2, X_valid2, y_valid2, False) # 開始訓練，不使用 cache\n",
    "    \n",
    "    print(\"Total training time: {:.1f}s\".format(time.time() - t0))\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, checkpoint_dir) # 從訓練過程中儲存的 ckpt 取回 model\n",
    "    acc_test = sess.run(accuracy, feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####\n",
    "#\n",
    "#     Homework 3.4 Bonus\n",
    "#\n",
    "#\n",
    "#     基本上結果會是：Final test accuracy: 88.75% 上下，比 3.3 進步 1%。\n",
    "#\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"save/Team07_HW2.ckpt.meta\")            # 回復 HW2 完成的 graph 及其 model\n",
    "\n",
    "X         = tf.get_default_graph().get_tensor_by_name(\"X:0\")                       # 利用 name 抓出 tf 變數\n",
    "y         = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss      = tf.get_default_graph().get_tensor_by_name(\"calc_loss/loss:0\")          # 因為在 HW2 中有用到 with scope，因此有前綴\n",
    "accuracy  = tf.get_default_graph().get_tensor_by_name(\"calc_accuracy/accuracy:0\")\n",
    "\n",
    "hidden4   = tf.get_default_graph().get_tensor_by_name(\"DNN/hidden4/Elu:0\")\n",
    "logits    = tf.layers.dense(hidden4, 5, name=\"new_logits\")\n",
    "Y_proba   = tf.nn.softmax(logits)\n",
    "\n",
    "entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[12]|new_logits\") # 可從所有變數中抓出可以更新（訓練）的變數，利用 scope 抓出該層的變數\n",
    "optimizer         = tf.train.AdamOptimizer(learning_rate=0.01, name=\"Adam2\")\n",
    "training_op       = optimizer.minimize(loss, var_list=output_layer_vars)                # 再從 optimizer 這裡限制要被更新的變數有哪些\n",
    "\n",
    "correct  = tf.equal(tf.argmax(logits, 1), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "saver  = tf.train.Saver()\n",
    "config = tf.ConfigProto(device_count = {'GPU': 1}) # 指定 GPU id，限制使用它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/Team07_HW2.ckpt\n",
      "0\tValidation loss: 0.554901\tBest loss: 0.554901\tAccuracy: 86.00%\n",
      "1\tValidation loss: 0.457852\tBest loss: 0.457852\tAccuracy: 88.67%\n",
      "2\tValidation loss: 0.655102\tBest loss: 0.457852\tAccuracy: 85.33%\n",
      "3\tValidation loss: 0.489796\tBest loss: 0.457852\tAccuracy: 86.00%\n",
      "4\tValidation loss: 0.509009\tBest loss: 0.457852\tAccuracy: 90.00%\n",
      "5\tValidation loss: 0.489723\tBest loss: 0.457852\tAccuracy: 92.67%\n",
      "6\tValidation loss: 0.463676\tBest loss: 0.457852\tAccuracy: 93.33%\n",
      "7\tValidation loss: 0.474216\tBest loss: 0.457852\tAccuracy: 92.67%\n",
      "8\tValidation loss: 0.479849\tBest loss: 0.457852\tAccuracy: 93.33%\n",
      "9\tValidation loss: 0.591258\tBest loss: 0.457852\tAccuracy: 90.00%\n",
      "10\tValidation loss: 0.538405\tBest loss: 0.457852\tAccuracy: 92.00%\n",
      "11\tValidation loss: 0.643287\tBest loss: 0.457852\tAccuracy: 90.00%\n",
      "12\tValidation loss: 0.634465\tBest loss: 0.457852\tAccuracy: 90.67%\n",
      "13\tValidation loss: 0.682020\tBest loss: 0.457852\tAccuracy: 84.67%\n",
      "14\tValidation loss: 0.972135\tBest loss: 0.457852\tAccuracy: 86.67%\n",
      "15\tValidation loss: 0.831119\tBest loss: 0.457852\tAccuracy: 90.00%\n",
      "16\tValidation loss: 0.895854\tBest loss: 0.457852\tAccuracy: 88.67%\n",
      "17\tValidation loss: 0.577693\tBest loss: 0.457852\tAccuracy: 92.67%\n",
      "18\tValidation loss: 0.715455\tBest loss: 0.457852\tAccuracy: 91.33%\n",
      "19\tValidation loss: 0.521975\tBest loss: 0.457852\tAccuracy: 94.00%\n",
      "20\tValidation loss: 0.734364\tBest loss: 0.457852\tAccuracy: 92.67%\n",
      "21\tValidation loss: 0.982631\tBest loss: 0.457852\tAccuracy: 89.33%\n",
      "Early stopping!\n",
      "Total training time: 3.1s\n",
      "INFO:tensorflow:Restoring parameters from save/Team07_HW3_4.ckpt\n",
      "Final test accuracy: 88.75%\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir=\"save/Team07_HW3_4.ckpt\"\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    restore_saver.restore(sess, \"save/Team07_HW2.ckpt\") # 從 HW2 存的 ckpt 中回復訓練好的變數\n",
    "    for var in output_layer_vars:                       # 針對會被更新的變數做 initial\n",
    "        var.initializer.run()\n",
    "\n",
    "    t0 = time.time()\n",
    "    run(sess, X_train2, y_train2, X_valid2, y_valid2, False) # 開始訓練，不使用 cache\n",
    "    \n",
    "    print(\"Total training time: {:.1f}s\".format(time.time() - t0))\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, checkpoint_dir) # 從訓練過程中儲存的 ckpt 取回 model\n",
    "    acc_test = sess.run(accuracy, feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
